{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practice #2: Training Convolutional Neural Networks\n",
    "\n",
    "Now, you're going to leave behind your implementations and instead migrate to one of popular deep learning frameworks, **TensorFlow**. <br>\n",
    "In this notebook, you will learn how to train convolutional neural networks (CNNs) for classifying images in the CIFAR-10 dataset. <br>\n",
    "There are **3 sections**, and in each section, you need to follow the instructions to complete the skeleton codes and explain them.\n",
    "\n",
    "1. [Training a simple CNN model](#1) ( 10 points )\n",
    "2. [Training a CNN model with Inception modules](#2) ( 10 points )\n",
    "3. [Design a better model on CIFAR-10](#3) ( 20 points )  \n",
    "\n",
    "**Note**: certain details are missing or ambiguous on purpose, in order to test your knowledge on the related materials. However, if you really feel that something essential is missing and cannot proceed to the next step, then contact the teaching staff with clear description of your problem.\n",
    "\n",
    "### Some helpful tutorials and references for practice #2:\n",
    "- [1] TensorFlow official tutorials. [[link]](https://www.tensorflow.org/get_started/get_started)\n",
    "- [2] Stanford CS231n lectures. [[link]](http://cs231n.stanford.edu/)\n",
    "- [3] Iandola et al, \"SqueezeNet: AlexNet-level accuracy with 50x fewer parameters and < 0.5MB model size\", arXiv 2016. [[pdf]](https://arxiv.org/pdf/1602.07360.pdf)\n",
    "- [4] Simonyan et al., \"Deep Inside Convolutional Networks: Visualising Image Classification Models and Saliency Maps\", ICLR Workshop 2014. [[pdf]](https://arxiv.org/pdf/1312.6034.pdf)\n",
    "- [5] Szegedy et al., \"Intriguing properties of neural networks\", ICLR 2014. [[pdf]](https://arxiv.org/pdf/1312.6199.pdf)\n",
    "- [6] Szegedy et al., \"Going deeper with convolutions\", CVPR 2015. [[pdf]](http://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Szegedy_Going_Deeper_With_2015_CVPR_paper.pdf)\n",
    "- [7] Yosinski et al., \"Understanding Neural Networks Through Deep Visualization\", ICML Workshop 2015. [[pdf]](http://yosinski.com/media/papers/Yosinski__2015__ICML_DL__Understanding_Neural_Networks_Through_Deep_Visualization__.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load datasets\n",
    "The CIFAR-10 dataset will be downloaded automatically if it is not located in the *Utils* directory. <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from Utils.data_utils import load_CIFAR10, plot_images\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (12.0, 6.0)\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "    \n",
    "conf = tf.ConfigProto()\n",
    "# conf.gpu_options.per_process_gpu_memory_fraction = 0.2\n",
    "# os.environ['CUDA_VISIBLE_DEVICES'] = '0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data has already been downloaded and unpacked.\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './Utils/cifar-10-bathes-py/batches.meta'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-aeeadfdde398>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mClass_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_CIFAR10\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Train data shape '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Train labels shape '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Validation data shape '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_val\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Validataion labels shape '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY_val\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/ML/seil/SK/cnn2/Utils/data_utils.py\u001b[0m in \u001b[0;36mload_CIFAR10\u001b[0;34m(val_batch)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m     \u001b[0;31m# Load the class-names\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 129\u001b[0;31m     \u001b[0mClass_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_unpickle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"./Utils/cifar-10-bathes-py/batches.meta\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'label_names'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    130\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mClass_names\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/ML/seil/SK/cnn2/Utils/data_utils.py\u001b[0m in \u001b[0;36m_unpickle\u001b[0;34m(file)\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_unpickle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m   \u001b[0mfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m   \u001b[0mdict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'latin1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m   \u001b[0mfo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './Utils/cifar-10-bathes-py/batches.meta'"
     ]
    }
   ],
   "source": [
    "X_train, Y_train, X_val, Y_val, X_test, Y_test, Class_names = load_CIFAR10()\n",
    "print('Train data shape ' + str(X_train.shape))\n",
    "print('Train labels shape ' + str(Y_train.shape))\n",
    "print('Validation data shape ' + str(X_val.shape))\n",
    "print('Validataion labels shape ' + str(Y_val.shape))\n",
    "print('Test data shape ' + str(X_test.shape))\n",
    "print('Test labels shape ' + str(Y_test.shape))\n",
    "plot_images(X_train, Y_train, Class_names, Each_Category=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a name=\"1\"></a> 1. Training a basic CNN model ( 10 points )\n",
    "\n",
    "In this section, you will learn how to define a simple CNN model architecture, train the model, and evaluate its performance on the validation dataset. The goal here isn't to get good performance (that'll be next), but instead to get comfortable with understanding the TensorFlow and configuring a CNN model.\n",
    "\n",
    "Using the code provided as guidance, **(1) define**, **(2) train**, and **(3) evaluate** a model with the following architecture and training setup:\n",
    "\n",
    "#### CNN architecture:\n",
    "* 9x9 Convolutional layer with 8 filters, strides of 1, and ReLU activation\n",
    "* 3x3 Max pooling layer with strides of 3\n",
    "* Fully connected layer with 8 output units and ReLU activation\n",
    "* Fully connected layer with 10 output units and linear activation\n",
    "\n",
    "#### Training setup:\n",
    "* Loss function: Sotfmax cross entropy\n",
    "* Optimizer: Gradient descent with 0.01 learning rate\n",
    "* Batch size: 32\n",
    "* Training epoch: 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01 \t Average loss: 2.0041\n",
      "Training done!\n",
      "(Loss, Accuracy) on Training Dataset (1.7322, 0.37)\n",
      "(Loss, Accuracy) on Validataion Dataset (1.7418, 0.36)\n"
     ]
    }
   ],
   "source": [
    "# Define our model as a class\n",
    "# It includes our model architecture, loss function, optimizer, and evaluation metrics\n",
    "class simple_model(object):\n",
    "    def __init__(self):\n",
    "        ##############################################################################\n",
    "        #                          IMPLEMENT YOUR CODE                               #\n",
    "        ##############################################################################\n",
    "        # Define input variables \n",
    "        self.inputs = tf.placeholder(tf.float32, [None, 32, 32, 3])\n",
    "        self.targets = tf.placeholder(tf.int64, [None])\n",
    "        self.is_training = tf.placeholder(tf.bool)\n",
    "\n",
    "        # Define model architecture\n",
    "        with tf.variable_scope('L1'):\n",
    "            L1 = tf.layers.conv2d(inputs=self.inputs, filters=8,\\\n",
    "                kernel_size=[9, 9], strides=[1,1], padding='valid',\\\n",
    "                activation=tf.nn.relu)\n",
    "            L1 = tf.layers.max_pooling2d(inputs=L1, pool_size=[3, 3], \\\n",
    "                strides=[3, 3], padding='valid')\n",
    "        with tf.variable_scope('L2'):\n",
    "            L2 = tf.contrib.layers.flatten(L1)\n",
    "            L2 = tf.layers.dense(L2, 8, activation=tf.nn.relu)\n",
    "        with tf.variable_scope('Output'):\n",
    "            self.outputs = tf.layers.dense(L2, 10, activation=None)\n",
    "        \n",
    "        # Define loss function and optimizer\n",
    "        self.loss = tf.reduce_mean(\n",
    "            tf.nn.softmax_cross_entropy_with_logits(\n",
    "                logits=self.outputs, \n",
    "                labels=tf.one_hot(self.targets,10)))\n",
    "        self.optimizer = tf.train.\n",
    "        GradientDescentOptimizer(0.01).minimize(self.loss)\n",
    "        \n",
    "        # Variables for evaluation\n",
    "        self.corr = tf.equal(\n",
    "            self.targets, tf.argmax(self.outputs,1))\n",
    "        self.accuracy = tf.reduce_mean(\n",
    "            tf.cast(self.corr, tf.float32))   \n",
    "        ##############################################################################\n",
    "        #                             END OF YOUR CODE                               #\n",
    "        ##############################################################################\n",
    "\n",
    "# Create a function to train and evaluata a model\n",
    "# You can reuse this function throughout the assignment\n",
    "def run_model(session, model, X, Y, epochs=5, batch_size=32, is_training=False):\n",
    "    # For training the model\n",
    "    if is_training:\n",
    "        ##############################################################################\n",
    "        #                          IMPLEMENT YOUR CODE                               #\n",
    "        ##############################################################################\n",
    "        for epoch in range(epochs):\n",
    "            total_loss = 0\n",
    "            # Iterate over the entire dataset\n",
    "            for i in range(int(X.shape[0] / batch_size)):\n",
    "                _, cost_val = sess.run([model.optimizer, model.loss], \n",
    "                                       feed_dict={model.inputs: X[i*batch_size:(i+1)*batch_size],\n",
    "                                                  model.targets: Y[i*batch_size:(i+1)*batch_size],\n",
    "                                                  model.is_training: is_training})\n",
    "                total_loss += cost_val\n",
    "            print(\"Epoch: %02d \\t Average loss: %.4f\" % (epoch+1, total_loss / X.shape[0] * batch_size))\n",
    "        ##############################################################################\n",
    "        #                             END OF YOUR CODE                               #\n",
    "        ##############################################################################\n",
    "        print(\"Training done!\")\n",
    "\n",
    "    # Evaluate loss and accuracy of the model\n",
    "    else:\n",
    "        ##############################################################################\n",
    "        #                          IMPLEMENT YOUR CODE                               #\n",
    "        ##############################################################################\n",
    "        loss, accuracy = 0.0, 0.0\n",
    "        for i in range(int(X.shape[0] / batch_size)):\n",
    "            l, a = sess.run([model.loss, model.accuracy],\n",
    "                            feed_dict={model.inputs: X[i*batch_size:(i+1)*batch_size],\n",
    "                                       model.targets: Y[i*batch_size:(i+1)*batch_size],\n",
    "                                       model.is_training: is_training})\n",
    "            loss += l\n",
    "            accuracy += a\n",
    "        ##############################################################################\n",
    "        #                             END OF YOUR CODE                               #\n",
    "        ##############################################################################\n",
    "        return (loss / X.shape[0] * batch_size, accuracy / X.shape[0] * batch_size)\n",
    "\n",
    "# Clear old variables\n",
    "tf.reset_default_graph()    \n",
    "\n",
    "# Declare out simple model\n",
    "model = simple_model()    \n",
    "    \n",
    "# Now, train and evaluate the model\n",
    "with tf.Session(config=conf) as sess:\n",
    "    ##############################################################################\n",
    "    #                          IMPLEMENT YOUR CODE                               #\n",
    "    ##############################################################################\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    run_model(sess, model, X_train, Y_train, epochs=1, is_training=True)\n",
    "    ##############################################################################\n",
    "    #                             END OF YOUR CODE                               #\n",
    "    ##############################################################################\n",
    "    print(\"(Loss, Accuracy) on Training Dataset (%.4f, %.2f)\" % run_model(sess, model, X_train, Y_train))\n",
    "    print(\"(Loss, Accuracy) on Validataion Dataset (%.4f, %.2f)\" % run_model(sess, model, X_val, Y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a name=\"2\"></a>2. Training a CNN model with Inception module ( 10 points )\n",
    "\n",
    "In this section, you will implement a little more complex CNN model with an `Inception module` [6]. <br>\n",
    "\n",
    "![Inception_module](Utils/InceptionModule_GoogLeNet.png)\n",
    "\n",
    "Using the code provided as guidance, <br>\n",
    "**(1)** Define an `inception module`, which is the building block of **Inception model (a.k.a GoogLeNet)**, the winner of ILSVRC14. <br>\n",
    "**(2)** Define, train, and evaluate a CNN model with the following architecture and training setup:\n",
    "\n",
    "#### CNN architecture:\n",
    "* `Inception module` with \n",
    "    * 8 filters for the main convolutions (blue blocks in the Figure(a))\n",
    "    * 2 filters for the dimensionality reduction convolutions (yellow blocks in the Figure(a))\n",
    "    *  ReLU activation\n",
    "* Fully connected layer with 10 output units and linear activation\n",
    "\n",
    "#### Training setup:\n",
    "* Loss function: Sotfmax cross entropy\n",
    "* Optimizer: Gradient descent with 0.01 learning rate\n",
    "* Batch size: 32\n",
    "* Training epoch: 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01 \t Average loss: 1.6239\n",
      "Epoch: 02 \t Average loss: 1.3630\n",
      "Epoch: 03 \t Average loss: 1.2226\n",
      "Epoch: 04 \t Average loss: 1.1229\n",
      "Epoch: 05 \t Average loss: 1.0383\n",
      "Training done!\n",
      "(Loss, Accuracy) on Training Dataset (0.9519, 0.67)\n",
      "(Loss, Accuracy) on Validataion Dataset (1.2408, 0.57)\n"
     ]
    }
   ],
   "source": [
    "def Inception_module(Input, C1, C3_R, C3, C5_R, C5, P3_R):\n",
    "    ##############################################################################\n",
    "    #                          IMPLEMENT YOUR CODE                               #\n",
    "    ##############################################################################\n",
    "    conv1 = tf.layers.conv2d(Input, C1, [1, 1], [1, 1], padding='same', activation=tf.nn.relu)\n",
    "    conv3_R = tf.layers.conv2d(Input, C3_R, [1, 1], [1, 1], padding='same', activation=tf.nn.relu)\n",
    "    conv3 = tf.layers.conv2d(conv3_R, C3, [3, 3], [1, 1], padding='same', activation=tf.nn.relu)\n",
    "    conv5_R = tf.layers.conv2d(Input, C5_R, [1, 1], [1, 1], padding='same', activation=tf.nn.relu)\n",
    "    conv5 = tf.layers.conv2d(conv5_R, C5, [5, 5], [1, 1], padding='same', activation=tf.nn.relu)\n",
    "    pool3 = tf.layers.max_pooling2d(Input, [3, 3], [1, 1], padding='same')\n",
    "    pool3_R = tf.layers.conv2d(pool3, P3_R, [1, 1], [1, 1], padding='same', activation=tf.nn.relu)\n",
    "    Inception = tf.concat([conv1, conv3, conv5, pool3_R], axis=3)\n",
    "    ##############################################################################\n",
    "    #                             END OF YOUR CODE                               #\n",
    "    ##############################################################################\n",
    "    return Inception\n",
    "\n",
    "class complex_model(object):\n",
    "    def __init__(self):\n",
    "        ##############################################################################\n",
    "        #                          IMPLEMENT YOUR CODE                               #\n",
    "        ##############################################################################\n",
    "        self.inputs = tf.placeholder(tf.float32, [None, 32, 32, 3])\n",
    "        self.targets = tf.placeholder(tf.int64, [None])\n",
    "        self.is_training = tf.placeholder(tf.bool)\n",
    "\n",
    "        with tf.variable_scope('L1'):\n",
    "            L1 = Inception_module(self.inputs, 8, 2, 8, 2, 8, 2)      \n",
    "        \n",
    "        with tf.variable_scope('Output'):\n",
    "            F = tf.contrib.layers.flatten(L1) \n",
    "            self.outputs = tf.layers.dense(F, 10, activation=None)\n",
    "        \n",
    "        # Define loss function and optimizer\n",
    "        self.loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=self.outputs, labels=tf.one_hot(self.targets,10)))\n",
    "        self.optimizer = tf.train.GradientDescentOptimizer(0.01).minimize(self.loss)\n",
    "        \n",
    "        # Variables for evaluation\n",
    "        self.corr = tf.equal(self.targets, tf.argmax(self.outputs,1))\n",
    "        self.accuracy = tf.reduce_mean(tf.cast(self.corr, tf.float32))   \n",
    "        ##############################################################################\n",
    "        #                             END OF YOUR CODE                               #\n",
    "        ############################################################################## \n",
    "\n",
    "# Clear old variables\n",
    "tf.reset_default_graph() \n",
    "\n",
    "# Declare out simple model\n",
    "model = complex_model()\n",
    "\n",
    "# Now, create a tf.Session and train the model\n",
    "with tf.Session(config=conf) as sess:\n",
    "    ##############################################################################\n",
    "    #                          IMPLEMENT YOUR CODE                               #\n",
    "    ##############################################################################\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    run_model(sess, model, X_train, Y_train, epochs=5, is_training=True)\n",
    "    ##############################################################################\n",
    "    #                             END OF YOUR CODE                               #\n",
    "    ##############################################################################\n",
    "    print(\"(Loss, Accuracy) on Training Dataset (%.4f, %.2f)\" % run_model(sess, model, X_train, Y_train))\n",
    "    print(\"(Loss, Accuracy) on Validataion Dataset (%.4f, %.2f)\" % run_model(sess, model, X_val, Y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a name=\"3\"></a>3. Design a better model on CIFAR-10\n",
    "\n",
    "Now it's your job to experiment with CNNs to train a model that achieves **<font color=red>>= 70% accuracy on the validation set</font>** of CIFAR-10. <br> You can reuse the implemented functions from above.\n",
    "\n",
    "### Things you can try to change:\n",
    "- Filter size\n",
    "- Number of filters\n",
    "- Pooling vs Strided Convolution\n",
    "- Network architectures\n",
    "- Optimizers\n",
    "- Activation functions\n",
    "- Regularizations\n",
    "- Model ensembles\n",
    "- Data augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01 \t Average loss: 1.4107\n",
      "Epoch: 02 \t Average loss: 0.9871\n",
      "Epoch: 03 \t Average loss: 0.8227\n",
      "Epoch: 04 \t Average loss: 0.7079\n",
      "Epoch: 05 \t Average loss: 0.6190\n",
      "Epoch: 06 \t Average loss: 0.5490\n",
      "Epoch: 07 \t Average loss: 0.4986\n",
      "Epoch: 08 \t Average loss: 0.4327\n",
      "Epoch: 09 \t Average loss: 0.3837\n",
      "Epoch: 10 \t Average loss: 0.3574\n",
      "Training done!\n",
      "(Loss, Accuracy) on Training Dataset (0.3956, 0.86)\n",
      "(Loss, Accuracy) on Validataion Dataset (1.0951, 0.70)\n"
     ]
    }
   ],
   "source": [
    "class my_model(object):\n",
    "    def __init__(self):\n",
    "        ##############################################################################\n",
    "        #                          IMPLEMENT YOUR CODE                               #\n",
    "        ##############################################################################\n",
    "        self.inputs = tf.placeholder(tf.float32, [None, 32, 32, 3])\n",
    "        self.targets = tf.placeholder(tf.int64, [None])\n",
    "        self.is_training = tf.placeholder(tf.bool)\n",
    "\n",
    "        with tf.variable_scope('L1'):\n",
    "            L1 = Inception_module(self.inputs, 32, 4, 32, 4, 32, 4)      \n",
    "            L1 = tf.layers.max_pooling2d(L1, pool_size=[3, 3], strides=[2, 2], padding='valid')\n",
    "            L1 = tf.layers.dropout(L1, 0.7, self.is_training)\n",
    "            \n",
    "        with tf.variable_scope('L2'):\n",
    "            L2 = Inception_module(L1, 64, 8, 64, 8, 64, 8)      \n",
    "            L2 = tf.layers.max_pooling2d(L2, pool_size=[3, 3], strides=[2, 2], padding='valid')\n",
    "            L2 = tf.layers.dropout(L2, 0.7, self.is_training)\n",
    "        \n",
    "        with tf.variable_scope('L3'):\n",
    "            L3 = tf.contrib.layers.flatten(L2)\n",
    "            L3 = tf.layers.dense(L3, 64, activation=tf.nn.relu)\n",
    "            L3 = tf.layers.dropout(L3, 0.7, self.is_training)\n",
    "        \n",
    "        with tf.variable_scope('Output'):\n",
    "            self.outputs = tf.layers.dense(L3, 10, activation=None)\n",
    "        \n",
    "        # Define loss function and optimizer\n",
    "        self.loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=self.outputs, labels=tf.one_hot(self.targets,10)))\n",
    "        self.optimizer = tf.train.AdamOptimizer().minimize(self.loss)\n",
    "        \n",
    "        # Variables for evaluation\n",
    "        self.corr = tf.equal(self.targets, tf.argmax(self.outputs,1))\n",
    "        self.accuracy = tf.reduce_mean(tf.cast(self.corr, tf.float32))   \n",
    "        ##############################################################################\n",
    "        #                             END OF YOUR CODE                               #\n",
    "        ##############################################################################\n",
    "\n",
    "# Clear old variables\n",
    "tf.reset_default_graph()    \n",
    "\n",
    "# Declare out simple model\n",
    "model = my_model()    \n",
    "    \n",
    "# Now, create a tf.Session and train the model\n",
    "with tf.Session(config=conf) as sess:\n",
    "    ##############################################################################\n",
    "    #                          IMPLEMENT YOUR CODE                               #\n",
    "    ##############################################################################\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    run_model(sess, model, X_train, Y_train, epochs=10, batch_size=100, is_training=True)\n",
    "    ##############################################################################\n",
    "    #                             END OF YOUR CODE                               #\n",
    "    ##############################################################################\n",
    "    print(\"(Loss, Accuracy) on Training Dataset (%.4f, %.2f)\" % run_model(sess, model, X_train, Y_train))\n",
    "    print(\"(Loss, Accuracy) on Validataion Dataset (%.4f, %.2f)\" % run_model(sess, model, X_val, Y_val))\n",
    "    \n",
    "    #Save your final model\n",
    "    saver = tf.train.Saver()\n",
    "    saver.save(sess, \"./Utils/model_checkpoints/my_model_final\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Describe what you did here\n",
    "In this cell you should also write an explanation of what you did, any additional features that you implemented, and any visualizations or graphs that you make in the process of training and evaluating your network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Tell us here_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Set - Do this only once\n",
    "Now that you've gotten a result that you're happy with, test your final model on the test set. This would be the score you would achieve on a competition. Think about how this compares to your validation set accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./Utils/model_checkpoints/my_model_final\n",
      "(Loss, Accuracy) on Test Dataset (1.0912, 0.70)\n"
     ]
    }
   ],
   "source": [
    "# Clear old variables\n",
    "tf.reset_default_graph()  \n",
    "\n",
    "with tf.Session(config=conf) as sess:\n",
    "    #Load your final model\n",
    "    model = my_model()\n",
    "    saver = tf.train.Saver()\n",
    "    saver.restore(sess, \"./Utils/model_checkpoints/my_model_final\")\n",
    "    print(\"(Loss, Accuracy) on Test Dataset (%.4f, %.2f)\" % run_model(sess, model, X_test, Y_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ML]",
   "language": "python",
   "name": "conda-env-ML-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
